<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>reclab.recommenders.cfnade.cfnade_lib.nade API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>reclab.recommenders.cfnade.cfnade_lib.nade</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import tensorflow as tf
from keras.engine import Layer, InputSpec
from keras import backend as K
from keras import initializers
from keras import regularizers
from keras import constraints

# def dot_product(x, kernel):
#     &#34;&#34;&#34;
#     Wrapper for dot product operation, in order to be compatible with both
#     Theano and Tensorflow
#     Args:
#         x (): input
#         kernel (): weights
#     Returns:
#     &#34;&#34;&#34;
#     return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)


class NADE(Layer):
    def __init__(self,
                 hidden_dim,
                 activation,
                 W_regularizer=None,
                 V_regularizer=None,
                 b_regularizer=None,
                 c_regularizer=None,
                 bias=False, 
                 normalized_layer=False,
                 **kwargs):

        self.init = initializers.get(&#39;uniform&#39;)

        self.bias = bias
        self.activation = activation
        self.hidden_dim = hidden_dim

        self.W_regularizer = regularizers.get(W_regularizer)
        self.V_regularizer = regularizers.get(V_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)
        self.c_regularizer = regularizers.get(c_regularizer)
        
        self.normalized_layer = normalized_layer

        super(NADE, self).__init__(**kwargs)

    def build(self, input_shape):
        self.input_dim1 = input_shape[1]
        self.input_dim2 = input_shape[2]

        self.W = self.add_weight(
            shape=(self.input_dim1, self.input_dim2, self.hidden_dim),
            initializer=self.init,
            name=&#39;{}_W&#39;.format(self.name),
            regularizer=self.W_regularizer)
        if self.bias:
            self.c = self.add_weight(
                shape=(self.hidden_dim, ),
                initializer=self.init,
                name=&#39;{}_c&#39;.format(self.name),
                regularizer=self.c_regularizer)

        if self.bias:
            self.b = self.add_weight(
                shape=(self.input_dim1, self.input_dim2),
                initializer=self.init,
                name=&#39;{}_b&#39;.format(self.name),
                regularizer=self.b_regularizer)

        self.V = self.add_weight(
            shape=(self.hidden_dim, self.input_dim1, self.input_dim2),
            initializer=self.init,
            name=&#39;{}_V&#39;.format(self.name),
            regularizer=self.V_regularizer)

        super().build(input_shape)

    def call(self, original_x):

        x = K.cumsum(original_x[:, :, ::-1], axis=2)[:, :, ::-1]
        # x.shape = (?,6040,5)
        # W.shape = (6040, 5, 500)
        # c.shape = (500,)
        output_ = tf.tensordot(x, self.W, axes=[[1, 2], [0, 1]])
       
        if self.normalized_layer:
            output_ /= tf.matmul(
                tf.maximum(
                    tf.reshape(
                        tf.reduce_sum(
                            tf.reduce_sum(original_x, axis=2), axis=1),
                        [-1, 1]), 1), tf.ones([1, output_.shape[1]]))

        if self.bias:
            output_ = output_ + self.c

        h_out = tf.reshape(output_, [-1, self.hidden_dim])
        #tf.cast(indices, tf.float32)
        # output_.shape = (?,500)

        h_out_act = K.tanh(h_out)
        # h_out_act.shape = (?,500)
        # V.shape = (500, 6040, 5)
        # b.shape = (6040,5)
        if self.bias:
            output = tf.tensordot(h_out_act, self.V, axes=[[1], [0]]) + self.b
        else:
            output = tf.tensordot(h_out_act, self.V, axes=[[1], [0]])
        # output.shape = (?,6040,5)
        output = tf.reshape(output, [-1, self.input_dim1, self.input_dim2])
        return output

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[1], input_shape[2])</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="reclab.recommenders.cfnade.cfnade_lib.nade.NADE"><code class="flex name class">
<span>class <span class="ident">NADE</span></span>
<span>(</span><span>hidden_dim, activation, W_regularizer=None, V_regularizer=None, b_regularizer=None, c_regularizer=None, bias=False, normalized_layer=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base layer class.</p>
<h1 id="properties">Properties</h1>
<pre><code>input, output: Input/output tensor(s). Note that if the layer
    is used more than once (shared layer), this is ill-defined
    and will raise an exception. In such cases, use
    &lt;code&gt;layer.get\_input\_at(node\_index)&lt;/code&gt;.
input_mask, output_mask: Mask tensors. Same caveats apply as
    input, output.
input_shape: Shape tuple. Provided for convenience, but note
    that there may be cases in which this attribute is
    ill-defined (e.g. a shared layer with multiple input
    shapes), in which case requesting &lt;code&gt;input\_shape&lt;/code&gt; will raise
    an Exception. Prefer using
    &lt;code&gt;layer.get\_input\_shape\_at(node\_index)&lt;/code&gt;.
input_spec: List of InputSpec class instances
    each entry describes one required input:
        - ndim
        - dtype
    A layer with &lt;code&gt;n&lt;/code&gt; input tensors must have
    an &lt;code&gt;input\_spec&lt;/code&gt; of length &lt;code&gt;n&lt;/code&gt;.
name: String, must be unique within a model.
non_trainable_weights: List of variables.
output_shape: Shape tuple. See &lt;code&gt;input\_shape&lt;/code&gt;.
stateful: Boolean indicating whether the layer carries
    additional non-weight state. Used in, for instance, RNN
    cells to carry information between batches.
supports_masking: Boolean indicator of whether the layer
    supports masking, typically for unused timesteps in a
    sequence.
trainable: Boolean, whether the layer weights
    will be updated during training.
trainable_weights: List of variables.
uses_learning_phase: Whether any operation
    of the layer uses &lt;code&gt;K.in\_training\_phase()&lt;/code&gt;
    or &lt;code&gt;K.in\_test\_phase()&lt;/code&gt;.
weights: The concatenation of the lists trainable_weights and
    non_trainable_weights (in this order).
dtype:  Default dtype of the layers's weights.
</code></pre>
<h1 id="methods">Methods</h1>
<pre><code>call(x, mask=None): Where the layer's logic lives.
__call__(x, mask=None): Wrapper around the layer logic (&lt;code&gt;call&lt;/code&gt;).
    If x is a Keras tensor:
        - Connect current layer with last layer from tensor:
            &lt;code&gt;self.\_add\_inbound\_node(last\_layer)&lt;/code&gt;
        - Add layer to tensor history
    If layer is not built:
        - Build from x._keras_shape
compute_mask(x, mask)
compute_output_shape(input_shape)
count_params()
get_config()
get_input_at(node_index)
get_input_mask_at(node_index)
get_input_shape_at(node_index)
get_output_at(node_index)
get_output_mask_at(node_index)
get_output_shape_at(node_index)
get_weights()
set_weights(weights)
</code></pre>
<h1 id="class-methods">Class Methods</h1>
<pre><code>from_config(config)
</code></pre>
<h1 id="internal-methods">Internal methods:</h1>
<pre><code>_add_inbound_node(layer, index=0)
assert_input_compatibility()
build(input_shape)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NADE(Layer):
    def __init__(self,
                 hidden_dim,
                 activation,
                 W_regularizer=None,
                 V_regularizer=None,
                 b_regularizer=None,
                 c_regularizer=None,
                 bias=False, 
                 normalized_layer=False,
                 **kwargs):

        self.init = initializers.get(&#39;uniform&#39;)

        self.bias = bias
        self.activation = activation
        self.hidden_dim = hidden_dim

        self.W_regularizer = regularizers.get(W_regularizer)
        self.V_regularizer = regularizers.get(V_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)
        self.c_regularizer = regularizers.get(c_regularizer)
        
        self.normalized_layer = normalized_layer

        super(NADE, self).__init__(**kwargs)

    def build(self, input_shape):
        self.input_dim1 = input_shape[1]
        self.input_dim2 = input_shape[2]

        self.W = self.add_weight(
            shape=(self.input_dim1, self.input_dim2, self.hidden_dim),
            initializer=self.init,
            name=&#39;{}_W&#39;.format(self.name),
            regularizer=self.W_regularizer)
        if self.bias:
            self.c = self.add_weight(
                shape=(self.hidden_dim, ),
                initializer=self.init,
                name=&#39;{}_c&#39;.format(self.name),
                regularizer=self.c_regularizer)

        if self.bias:
            self.b = self.add_weight(
                shape=(self.input_dim1, self.input_dim2),
                initializer=self.init,
                name=&#39;{}_b&#39;.format(self.name),
                regularizer=self.b_regularizer)

        self.V = self.add_weight(
            shape=(self.hidden_dim, self.input_dim1, self.input_dim2),
            initializer=self.init,
            name=&#39;{}_V&#39;.format(self.name),
            regularizer=self.V_regularizer)

        super().build(input_shape)

    def call(self, original_x):

        x = K.cumsum(original_x[:, :, ::-1], axis=2)[:, :, ::-1]
        # x.shape = (?,6040,5)
        # W.shape = (6040, 5, 500)
        # c.shape = (500,)
        output_ = tf.tensordot(x, self.W, axes=[[1, 2], [0, 1]])
       
        if self.normalized_layer:
            output_ /= tf.matmul(
                tf.maximum(
                    tf.reshape(
                        tf.reduce_sum(
                            tf.reduce_sum(original_x, axis=2), axis=1),
                        [-1, 1]), 1), tf.ones([1, output_.shape[1]]))

        if self.bias:
            output_ = output_ + self.c

        h_out = tf.reshape(output_, [-1, self.hidden_dim])
        #tf.cast(indices, tf.float32)
        # output_.shape = (?,500)

        h_out_act = K.tanh(h_out)
        # h_out_act.shape = (?,500)
        # V.shape = (500, 6040, 5)
        # b.shape = (6040,5)
        if self.bias:
            output = tf.tensordot(h_out_act, self.V, axes=[[1], [0]]) + self.b
        else:
            output = tf.tensordot(h_out_act, self.V, axes=[[1], [0]])
        # output.shape = (?,6040,5)
        output = tf.reshape(output, [-1, self.input_dim1, self.input_dim2])
        return output

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[1], input_shape[2])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="reclab.recommenders.cfnade.cfnade_lib.nade.NADE.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates the layer weights.</p>
<p>Must be implemented on all layers that have weights.</p>
<h1 id="arguments">Arguments</h1>
<pre><code>input_shape: Keras tensor (future input to layer)
    or list/tuple of Keras tensors to reference
    for weight shape computations.
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build(self, input_shape):
    self.input_dim1 = input_shape[1]
    self.input_dim2 = input_shape[2]

    self.W = self.add_weight(
        shape=(self.input_dim1, self.input_dim2, self.hidden_dim),
        initializer=self.init,
        name=&#39;{}_W&#39;.format(self.name),
        regularizer=self.W_regularizer)
    if self.bias:
        self.c = self.add_weight(
            shape=(self.hidden_dim, ),
            initializer=self.init,
            name=&#39;{}_c&#39;.format(self.name),
            regularizer=self.c_regularizer)

    if self.bias:
        self.b = self.add_weight(
            shape=(self.input_dim1, self.input_dim2),
            initializer=self.init,
            name=&#39;{}_b&#39;.format(self.name),
            regularizer=self.b_regularizer)

    self.V = self.add_weight(
        shape=(self.hidden_dim, self.input_dim1, self.input_dim2),
        initializer=self.init,
        name=&#39;{}_V&#39;.format(self.name),
        regularizer=self.V_regularizer)

    super().build(input_shape)</code></pre>
</details>
</dd>
<dt id="reclab.recommenders.cfnade.cfnade_lib.nade.NADE.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, original_x)</span>
</code></dt>
<dd>
<div class="desc"><p>This is where the layer's logic lives.</p>
<h1 id="arguments">Arguments</h1>
<pre><code>inputs: Input tensor, or list/tuple of input tensors.
**kwargs: Additional keyword arguments.
</code></pre>
<h1 id="returns">Returns</h1>
<pre><code>A tensor or list/tuple of tensors.
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, original_x):

    x = K.cumsum(original_x[:, :, ::-1], axis=2)[:, :, ::-1]
    # x.shape = (?,6040,5)
    # W.shape = (6040, 5, 500)
    # c.shape = (500,)
    output_ = tf.tensordot(x, self.W, axes=[[1, 2], [0, 1]])
   
    if self.normalized_layer:
        output_ /= tf.matmul(
            tf.maximum(
                tf.reshape(
                    tf.reduce_sum(
                        tf.reduce_sum(original_x, axis=2), axis=1),
                    [-1, 1]), 1), tf.ones([1, output_.shape[1]]))

    if self.bias:
        output_ = output_ + self.c

    h_out = tf.reshape(output_, [-1, self.hidden_dim])
    #tf.cast(indices, tf.float32)
    # output_.shape = (?,500)

    h_out_act = K.tanh(h_out)
    # h_out_act.shape = (?,500)
    # V.shape = (500, 6040, 5)
    # b.shape = (6040,5)
    if self.bias:
        output = tf.tensordot(h_out_act, self.V, axes=[[1], [0]]) + self.b
    else:
        output = tf.tensordot(h_out_act, self.V, axes=[[1], [0]])
    # output.shape = (?,6040,5)
    output = tf.reshape(output, [-1, self.input_dim1, self.input_dim2])
    return output</code></pre>
</details>
</dd>
<dt id="reclab.recommenders.cfnade.cfnade_lib.nade.NADE.compute_output_shape"><code class="name flex">
<span>def <span class="ident">compute_output_shape</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<h1 id="arguments">Arguments</h1>
<pre><code>input_shape: Shape tuple (tuple of integers)
    or list of shape tuples (one per output tensor of the layer).
    Shape tuples can include None for free dimensions,
    instead of an integer.
</code></pre>
<h1 id="returns">Returns</h1>
<pre><code>An output shape tuple.
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_output_shape(self, input_shape):
    return (input_shape[0], input_shape[1], input_shape[2])</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="reclab.recommenders.cfnade.cfnade_lib" href="index.html">reclab.recommenders.cfnade.cfnade_lib</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="reclab.recommenders.cfnade.cfnade_lib.nade.NADE" href="#reclab.recommenders.cfnade.cfnade_lib.nade.NADE">NADE</a></code></h4>
<ul class="">
<li><code><a title="reclab.recommenders.cfnade.cfnade_lib.nade.NADE.build" href="#reclab.recommenders.cfnade.cfnade_lib.nade.NADE.build">build</a></code></li>
<li><code><a title="reclab.recommenders.cfnade.cfnade_lib.nade.NADE.call" href="#reclab.recommenders.cfnade.cfnade_lib.nade.NADE.call">call</a></code></li>
<li><code><a title="reclab.recommenders.cfnade.cfnade_lib.nade.NADE.compute_output_shape" href="#reclab.recommenders.cfnade.cfnade_lib.nade.NADE.compute_output_shape">compute_output_shape</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>